{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw6n6CLxqhE5",
        "outputId": "f6109c9e-6213-4c03-ade2-9a27a099bea3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFlcWbBRMwXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be39259-778b-4857-89fe-e8daa7477f26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hqq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# fix triton in colab\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia\n",
        "\n",
        "!git clone https://github.com/dvmazur/mixtral-offloading.git --quiet\n",
        "!cd mixtral-offloading && pip install -q -r requirements.txt\n",
        "\n",
        "!huggingface-cli download lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo \\\n",
        "--quiet --local-dir \\\n",
        "Mixtral-8x7B-Instruct-v0.1-offloading-demo\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"mixtral-offloading\")\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from hqq.core.quantize import BaseQuantizeConfig\n",
        "from huggingface_hub import snapshot_download\n",
        "from IPython.display import clear_output\n",
        "from tqdm.auto import trange\n",
        "from transformers import AutoConfig, AutoTokenizer\n",
        "from transformers.utils import logging as hf_logging\n",
        "\n",
        "from src.build_model import OffloadConfig, QuantConfig, build_model"
      ],
      "metadata": {
        "id": "LXWXL_c4M0fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "sF18OwrSONpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "quantized_model_name = \"lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "state_path = \"Mixtral-8x7B-Instruct-v0.1-offloading-demo\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(quantized_model_name)\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "##### Change this to 5 if you have only 12 GB of GPU VRAM #####\n",
        "offload_per_layer = 4\n",
        "# offload_per_layer = 5\n",
        "###############################################################\n",
        "\n",
        "num_experts = config.num_local_experts\n",
        "\n",
        "offload_config = OffloadConfig(\n",
        "    main_size=config.num_hidden_layers * (num_experts - offload_per_layer),\n",
        "    offload_size=config.num_hidden_layers * offload_per_layer,\n",
        "    buffer_size=4,\n",
        "    offload_per_layer=offload_per_layer,\n",
        ")\n",
        "\n",
        "attn_config = BaseQuantizeConfig(\n",
        "    nbits=4,\n",
        "    group_size=64,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "attn_config[\"scale_quant_params\"][\"group_size\"] = 256\n",
        "\n",
        "\n",
        "ffn_config = BaseQuantizeConfig(\n",
        "    nbits=2,\n",
        "    group_size=16,\n",
        "    quant_zero=True,\n",
        "    quant_scale=True,\n",
        ")\n",
        "quant_config = QuantConfig(ffn_config=ffn_config, attn_config=attn_config)\n",
        "\n",
        "\n",
        "model = build_model(\n",
        "    device=device,\n",
        "    quant_config=quant_config,\n",
        "    offload_config=offload_config,\n",
        "    state_path=state_path,\n",
        ")"
      ],
      "metadata": {
        "id": "mQrBAyqhM7hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "ldN0Qb0EwGzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val = pd.read_csv('MEDIQA-CORR-2024-MS-ValidationSet-1-Full.csv')\n",
        "print(f\"df_val shape: {df_val.shape}\")\n",
        "df_val['mixtral_error_flag'] = 0\n",
        "df_val['mixtral_reason'] = ''\n",
        "for i, row in df_val.iterrows():\n",
        "  if i<200:\n",
        "    continue\n",
        "  else:\n",
        "    print(f\"index={i}\",flush=True)\n",
        "    past_key_values = None\n",
        "    sequence = None\n",
        "    seq_len = 0\n",
        "    text = row['Text']\n",
        "    user_input=f\"\"\"You are an AI trained in medical knowledge. Below are examples of clinical texts (delimited by triple quotes) followed by analysis of whether there is a diagnostic error and, if so, the reason for error.\n",
        "    ####\n",
        "    Here are some examples:\n",
        "    Clinical Text: ```A 17-year-old boy comes to the physician because of body aches and sore throat for 1 week. He has no history of serious illness and takes no medications. He lives with his parents; they recently adopted a cat from an animal shelter. He is sexually active with one female partner, and they use condoms consistently. His temperature is 38.7 C (101.7 F), pulse is 99/min, and blood pressure is 110/72 mm Hg. Examination shows bilateral posterior cervical lymphadenopathy. The pharynx is red and swollen. Laboratory studies show:\n",
        "    Hemoglobin 15 g/dL\n",
        "    Leukocyte count 11,500/mm3\n",
        "    Segmented neutrophils 48%\n",
        "    Band forms 2%\n",
        "    Basophils 0.5%\n",
        "    Eosinophils 1%\n",
        "    Lymphocytes 45%\n",
        "    Monocytes 3.5%\n",
        "    When the patient's serum is added to a sample of horse erythrocytes, the cells aggregate together. The causal pathogen is cytomegalovirus.```\n",
        "    Output= {{\"Error\": \"yes\",\n",
        "    \"Reason\": \"the sentence stating 'The causal pathogen is cytomegalovirus' is incorrect. The correct medical diagnosis should be 'the causal pathogen is Epstein-Barr virus'.\"\n",
        "    }}\n",
        "    Clinical Text: ```A previously healthy 18-year-old woman comes to the emergency department because of diarrhea and abdominal cramps since the previous evening. She has had around 3Ã¢â‚¬â€œ4 episodes of watery stools. She feels nauseous and has vomited twice. She recollects eating out 2 days ago. She has been on a vegan diet for 6 months. She takes no medications and has not traveled anywhere recently. Her temperature is 36.8 (98.2 F), pulse is 73/min, and blood pressure is 110/70 mm Hg. Examination shows dry mucous membranes. Abdominal examination is unremarkable. Norovirus was determined to be the casual organism.```\n",
        "    Output={{\n",
        "    \"Error\": \"No\",\n",
        "    \"Reason\": \"the context given in text aligns with the diagnosis\"\n",
        "    }}\n",
        "    Clinical Text: ```A 5-year-old boy is brought to the emergency department by his grandmother because of difficulty breathing. Over the past two hours, the grandmother has noticed his voice getting progressively hoarser and occasionally muffled, with persistent drooling. He has not had a cough. The child recently immigrated from Africa, and the grandmother is unsure if his immunizations are up-to-date. He appears uncomfortable and is sitting up and leaning forward with his chin hyperextended. His temperature is 39.5 C (103.1 F), pulse is 110/min, and blood pressure is 90/70 mm Hg. Pulse oximetry on room air shows an oxygen saturation of 95%. Pulmonary examination shows inspiratory stridor and scattered rhonchi throughout both lung fields, along with poor air movement. Pharyngoscopy is ordered. ```\n",
        "    Output= {{\n",
        "      \"Error\":\"yes\",\n",
        "      \"Reason\": \"The clinical text says 'Pharyngoscopy is ordered' but it should have been 'Nasotracheal intubation is performed' as per the context given in clinical text\"\n",
        "    }}\n",
        "    Clinical Text:```A 28-year-old man is brought to the emergency department after being struck by a car an hour ago as he was crossing the street. He did not lose consciousness. He is complaining of pain in his right arm, forehead, and pelvis. He also has the urge to urinate, but has been unable to do so since the accident. He takes no medications. His temperature is 37.1 C (98.9 F), pulse is 72/min, respirations are 18/min, and blood pressure is 118/82 mm Hg. There are abrasions over his scalp and face and a 1x3 cm area of ecchymosis above his right eye. Abdominal examination shows suprapubic tenderness. There is a scant amount of blood at the urethral meatus. There is no cervical spinal tenderness. Musculoskeletal examination shows tenderness and ecchymosis over his right distal forearm. An x-ray of the pelvis shows a fracture of the pelvic ramus. Retrograde urethrogram was then performed. A CT scan of the head and neck show no abnormalities.```\n",
        "    Output = {{\n",
        "      \"Error\": \"No\",\n",
        "      \"Reason\": \"the context given in text aligns with the diagnosis.\"\n",
        "    }}\n",
        "    ###\n",
        "    Now, you are given below a new clinical text delimited by triple quotes. Carefully evaluate and analyse the information presented in clinical text such as symptoms, clinical examination findings, patient history and other details. Determine if any of the given sentences contain a diagnostic error or not. Use your knowledge and the context provided to make your assessment. Provide the output only in JSON Format with the following keys. Do not provide explanations or notes.\n",
        "    Error, Reason\n",
        "    ```\n",
        "    Clinical Text: {text}\n",
        "    ```\n",
        "    \"\"\"\n",
        "    user_entry = dict(role=\"user\", content=user_input)\n",
        "    input_ids = tokenizer.apply_chat_template([user_entry], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    if past_key_values is None:\n",
        "        attention_mask = torch.ones_like(input_ids)\n",
        "    else:\n",
        "        seq_len = input_ids.size(1) + past_key_values[0][0][0].size(1)\n",
        "        attention_mask = torch.ones([1, seq_len - 1], dtype=torch.int, device=device)\n",
        "\n",
        "    # print(\"Mixtral: \", end=\"\")\n",
        "    result = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        past_key_values=past_key_values,\n",
        "        # streamer=streamer,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=200,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        return_dict_in_generate=True,\n",
        "        output_hidden_states=True,\n",
        "      )\n",
        "    output_text = tokenizer.decode(result['sequences'][0], skip_special_tokens=True)\n",
        "    output_text = output_text.split('[/INST]')[1].strip()\n",
        "    error_index = output_text.find('\"Error\": \"') + len('\"Error\": \"')\n",
        "    error_end_index = output_text.find('\"', error_index + 1)\n",
        "    error = output_text[error_index:error_end_index]\n",
        "    reason_index = output_text.find('\"Reason\": \"') + len('\"Reason\": \"')\n",
        "    reason_end_index = output_text.find('\"', reason_index + 1)\n",
        "    reason = output_text[reason_index:reason_end_index]\n",
        "\n",
        "    # print(error)\n",
        "    # print(reason)\n",
        "    df_val.at[i, 'mixtral_error_flag'] = error\n",
        "    df_val.at[i,'mixtral_reason'] = reason\n",
        "\n",
        "    sequence = result[\"sequences\"]\n",
        "    past_key_values = result[\"past_key_values\"]\n",
        "\n",
        "    if(i%5==0):\n",
        "      print(\"saving the file\",flush=True)\n",
        "      df_val.to_csv('/content/drive/MyDrive/EA_ST/mixtral_predictions_200.csv',index=False)"
      ],
      "metadata": {
        "id": "v98P0m6ZvEKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}