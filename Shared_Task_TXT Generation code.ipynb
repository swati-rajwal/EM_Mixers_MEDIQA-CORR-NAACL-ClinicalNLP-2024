{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57d698a-4835-46b3-9f7c-51aedcc8ea4a",
   "metadata": {},
   "source": [
    "# Converting results in TXT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14db6695-2510-4aa7-ac88-dda24c39269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('case_1_test_set_predictions.csv')\n",
    "def normalize_to_binary(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip().lower()\n",
    "    if value in ['yes', '1', 1]:\n",
    "        return 1\n",
    "    elif value in ['no', '0', 0]:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['gpt_4_1106_preview_error_flag'] = df['gpt_4_1106_preview_error_flag'].apply(normalize_to_binary)\n",
    "\n",
    "df.rename(columns={'gpt_4_1106_preview_error_flag':'Error Flag Predicted', 'gpt_4_1106_preview_sentence_id': 'Error Sentence ID Predicted', \n",
    "                   'gpt_4_1106_preview_Corrected_Sentence':'Corrected Sentence Predicted'},inplace=True)\n",
    "\n",
    "df.drop(columns=['Text','Sentences', 'gpt_4_1106_preview_Error_Sentence'], inplace=True)\n",
    "df.sample(3)\n",
    "df.to_csv('case1_without_RAG_final_results.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f5d1007-cb97-4c4c-be47-75abce26a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3385220/3534557061.py:25: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Patient's symptoms are suspected to be due to acute schistosomiasis.' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'Corrected Sentence Predicted'] = row['gpt_4_1106_preview_Corrected_Sentence']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test_set_preds_with_RAG_28_march_final_0s_and_1s.csv')\n",
    "def normalize_to_binary(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip().lower()\n",
    "    if value in ['yes', '1', 1]:\n",
    "        return 1\n",
    "    elif value in ['no', '0', 0]:\n",
    "        return 0\n",
    "    else:\n",
    "        return None  # For values that do not match expected inputs\n",
    "\n",
    "df['gpt_4_1106_preview_error_flag'] = df['gpt_4_1106_preview_error_flag'].apply(normalize_to_binary)\n",
    "df['reevaluated_gpt_4_1106_preview_error_flag'] = df['reevaluated_gpt_4_1106_preview_error_flag'].apply(normalize_to_binary)\n",
    "\n",
    "df['Error Flag Predicted'] = 0\n",
    "df['Error Sentence ID Predicted'] = -1\n",
    "df['Corrected Sentence Predicted'] = np.nan\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['gpt_4_1106_preview_error_flag'] == 0:\n",
    "        continue\n",
    "    elif row['gpt_4_1106_preview_error_flag'] == 1 and row['reevaluated_gpt_4_1106_preview_error_flag'] == 1:\n",
    "        df.at[index, 'Error Flag Predicted'] = 1\n",
    "        df.at[index, 'Error Sentence ID Predicted'] = row['gpt_4_1106_preview_sentence_id']\n",
    "        df.at[index, 'Corrected Sentence Predicted'] = row['gpt_4_1106_preview_Corrected_Sentence']\n",
    "    elif row['gpt_4_1106_preview_error_flag'] == 1 and row['reevaluated_gpt_4_1106_preview_error_flag'] == 0:\n",
    "        continue\n",
    "df.drop(columns=['Text','Sentences','gpt_4_1106_preview_error_flag','gpt_4_1106_preview_sentence_id','gpt_4_1106_preview_Corrected_Sentence','gpt_4_1106_preview_Error_Sentence','en_core_sci_scibert',\n",
    "       'en_ner_bc5cdr_md', 'Common_Keywords_Longest', 'final_keyword','reevaluated_gpt_4_1106_preview_error_flag','reevaluated_gpt_4_1106_preview_Corrected_Sentence'], inplace=True)\n",
    "\n",
    "df.to_csv('test_set_preds_with_RAG_FINAL_RESULTS.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f5b0998-a46c-411a-890b-f7155225675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The submission format should follow the data format and consists of:\n",
    "\n",
    "[Text ID] [Error Flag] [Error sentence ID or -1 for texts without errors] [Corrected sentence or NA for texts without errors]\n",
    "'''\n",
    "import csv\n",
    "\n",
    "def csv_to_txt(input_csv, output_txt):\n",
    "    with open(input_csv, mode='r', encoding='utf-8') as infile, open(output_txt, mode='w', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            text_id = row['Text ID']\n",
    "            error_flag = row['Error Flag Predicted']\n",
    "            sentence_id = row['Error Sentence ID Predicted']\n",
    "            corrected_sentence = row['Corrected Sentence Predicted']\n",
    "            if corrected_sentence == \"\":\n",
    "                corrected_sentence = \"NA\"\n",
    "            else:\n",
    "                corrected_sentence = f\"\\\"{corrected_sentence}\\\"\"\n",
    "            line = f\"{text_id} {error_flag} {sentence_id} {corrected_sentence}\\n\"\n",
    "            outfile.write(line)\n",
    "\n",
    "# Usage\n",
    "csv_file = 'test_set_preds_with_RAG_FINAL_RESULTS.csv'  # Replace with your CSV file path\n",
    "txt_file = 'test_set_preds_with_RAG_FINAL_RESULTS.txt'  # The output TXT file path\n",
    "csv_to_txt(csv_file, txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19b87d-2411-489e-911e-757a7bf1c930",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Evaluation Script 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ec776b-fb14-4d83-af41-0eb986b0fd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rouge in ./.local/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /opt/modules/Python/3.11.5/lib/python3.11/site-packages (from rouge) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa671c11-5ef2-42ad-bf87-7b1b686f215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813bd319-6f3e-4f53-8bd7-eaa48f0f01dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Parsing Funcs #\n",
    "#################\n",
    "\n",
    "\n",
    "def parse_reference_file(filepath):\n",
    "    \"\"\"Parsing reference file path.\n",
    "\n",
    "    Returns:\n",
    "        reference_corrections (dict) {text_id: \"reference correction\"}\n",
    "        reference_flags (dict) {text_id: \"1 or 0 error flag\"}\n",
    "        reference_sent_id (dict) {text_id: \"error sentence id or -1\"}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    reference_corrections = {}\n",
    "    reference_flags = {}\n",
    "    reference_sent_id = {}\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text_id = row['Text ID']\n",
    "        corrected_sentence = row['Corrected Sentence']\n",
    "        \n",
    "        if not isinstance(corrected_sentence, str):\n",
    "            if math.isnan(corrected_sentence):\n",
    "                corrected_sentence = \"NA\"\n",
    "            else:\n",
    "                corrected_sentence = str(corrected_sentence)\n",
    "                corrected_sentence = corrected_sentence.replace(\"\\n\", \" \") \\\n",
    "                  .replace(\"\\r\", \" \").strip()\n",
    "                  \n",
    "        reference_corrections[text_id] = corrected_sentence\n",
    "        reference_flags[text_id] = str(row['Error Flag'])\n",
    "        reference_sent_id[text_id] = str(row['Error Sentence ID'])\n",
    "\n",
    "    return reference_corrections, reference_flags, reference_sent_id\n",
    "\n",
    "\n",
    "def parse_run_submission_file(filepath):\n",
    "    \n",
    "    file = open(filepath, 'r')\n",
    "\n",
    "    candidate_corrections = {}\n",
    "    predicted_flags = {}\n",
    "    candidate_sent_id = {}\n",
    "    \n",
    "    lines = file.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "            \n",
    "        if not re.fullmatch('[a-z0-9\\-]+\\s[0-9]+\\s\\-?[0-9]+\\s.+', line):\n",
    "            print(\"Invalid line: \", line)\n",
    "            continue\n",
    "            \n",
    "        # replacing consecutive spaces\n",
    "        line = re.sub('\\s+', line, ' ')\n",
    "        \n",
    "        # parsing\n",
    "        items = line.split()\n",
    "        text_id = items[0]\n",
    "        error_flag = items[1]\n",
    "        sentence_id = items[2]\n",
    "        corrected_sentence = ' '.join(items[3:]).strip()\n",
    "        \n",
    "        # debug - parsing check\n",
    "        # print(\"{} -- {} -- {} -- {}\".format(text_id, error_flag, sentence_id, corrected_sentence))\n",
    "\n",
    "        predicted_flags[text_id] = error_flag\n",
    "        candidate_sent_id[text_id] = sentence_id\n",
    "\n",
    "        # processing candidate corrections\n",
    "        # removing quotes\n",
    "\n",
    "        while corrected_sentence.startswith('\"') and len(corrected_sentence) > 1:\n",
    "            corrected_sentence = corrected_sentence[1:]\n",
    "            \n",
    "        while corrected_sentence.endswith('\"') and len(corrected_sentence) > 1:\n",
    "            corrected_sentence = corrected_sentence[:-1]\n",
    "                   \n",
    "        if error_flag == '0':\n",
    "            # enforcing \"NA\" in predicted non-errors (used for consistent/reliable eval)\n",
    "            candidate_corrections[text_id] = \"NA\"\n",
    "        else:\n",
    "            candidate_corrections[text_id] = corrected_sentence\n",
    "\n",
    "    return candidate_corrections, predicted_flags, candidate_sent_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "471bd2b5-7c8a-4e6a-9200-1551afcb15d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Results:\n",
      " {'Error Flags Accuracy': 0.6480836236933798, 'Error Sentence Detection Accuracy': 0.6376306620209059}\n",
      "\n",
      "NLG Eval Results:\n",
      " {'R1F_subset_check': 0.5496118784257538, 'R2F_subset_check': 0.4388062049061051, 'RLF_subset_check': 0.542764446929799, 'R1FC': 0.5162627100618931, 'R2FC': 0.48383178122687404, 'RLFC': 0.5142585837703941}\n",
      "\n",
      "{'total_texts': 574, 'reference_na': 255, 'total_system_texts': 574, 'system_provided_na': 355, 'system_provided_correct_na': 204}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Eval Funcs #\n",
    "##############\n",
    "\n",
    "def compute_accuracy(reference_flags, reference_sent_id, predicted_flags, candidate_sent_id):\n",
    "    # Error Flags Accuracy (missing predictions are counted as false)\n",
    "    matching_flags_nb = 0\n",
    "    \n",
    "    for text_id in reference_flags:\n",
    "        if text_id in predicted_flags and reference_flags[text_id] == predicted_flags[text_id]:\n",
    "            matching_flags_nb += 1\n",
    "            \n",
    "    flags_accuracy = matching_flags_nb / len(reference_flags)\n",
    "    \n",
    "    # Error Sentence Detection Accuracy (missing predictions are counted as false)\n",
    "    matching_sentence_nb = 0\n",
    "    \n",
    "    for text_id in reference_sent_id:\n",
    "        if text_id in candidate_sent_id and candidate_sent_id[text_id] == reference_sent_id[text_id]:\n",
    "            matching_sentence_nb += 1\n",
    "            \n",
    "    sent_accuracy = matching_sentence_nb / len(reference_sent_id)\n",
    "\n",
    "    return {\n",
    "        \"Error Flags Accuracy\": flags_accuracy,\n",
    "        \"Error Sentence Detection Accuracy\": sent_accuracy\n",
    "    }\n",
    "\n",
    "def increment_counter(counters, counter_name):\n",
    "    counters[counter_name] = counters[counter_name] + 1\n",
    "\n",
    "class NLGMetrics(object):\n",
    "\n",
    "    def __init__(self, metrics = ['ROUGE']):\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def compute(self, references, predictions, counters):\n",
    "        results = {}\n",
    "        \n",
    "        if 'ROUGE' in self.metrics:\n",
    "            rouge = Rouge() \n",
    "            rouge_scores = rouge.get_scores(predictions, references)\n",
    "                            \n",
    "            rouge1f_scores = []\n",
    "            rouge2f_scores = []\n",
    "            rougeLf_scores = []\n",
    "            \n",
    "            for i in range(len(references)):\n",
    "                r1f = rouge_scores[i][\"rouge-1\"][\"f\"]\n",
    "                r2f = rouge_scores[i][\"rouge-2\"][\"f\"]\n",
    "                rlf = rouge_scores[i][\"rouge-l\"][\"f\"]\n",
    "                \n",
    "                rouge1f_scores.append(r1f)\t\n",
    "                rouge2f_scores.append(r2f)\n",
    "                rougeLf_scores.append(rlf)\n",
    "                \n",
    "            # for checking comparison with composite\n",
    "            rouge1check = np.array(rouge1f_scores).mean()\n",
    "            rouge2check = np.array(rouge2f_scores).mean()\n",
    "            rougeLcheck = np.array(rougeLf_scores).mean()\n",
    "\n",
    "            results['R1F_subset_check'] = rouge1check\n",
    "            results['R2F_subset_check'] = rouge2check\n",
    "            results['RLF_subset_check'] = rougeLcheck\n",
    "            \n",
    "            ###############################\n",
    "            # Composite score computation #\n",
    "            ###############################\n",
    "            \n",
    "            \"\"\"\n",
    "            NLG METRIC on sentence vs. sentence cases + ones or zeros \n",
    "            when either the reference or the candidate correction is NA\n",
    "            \"\"\"\n",
    "            \n",
    "            rouge1score = np.array(rouge1f_scores).sum()\n",
    "            rouge2score = np.array(rouge2f_scores).sum()\n",
    "            rougeLscore = np.array(rougeLf_scores).sum()\n",
    "            \n",
    "            composite_score_rouge1 = (rouge1score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "            composite_score_rouge2 = (rouge2score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "            composite_score_rougeL = (rougeLscore + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "\n",
    "            results['R1FC'] = composite_score_rouge1\n",
    "            results['R2FC'] = composite_score_rouge2\n",
    "            results['RLFC'] = composite_score_rougeL\n",
    "\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def get_nlg_eval_data(reference_corrections, candidate_corrections, remove_nonprint = False):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    \n",
    "    counters = {\n",
    "        \"total_texts\": 0,\n",
    "        \"reference_na\": 0,\n",
    "        \"total_system_texts\": 0,\n",
    "        \"system_provided_na\": 0,\n",
    "        \"system_provided_correct_na\": 0,\n",
    "    }\n",
    "    \n",
    "    for text_id in reference_corrections:\n",
    "        increment_counter(counters, \"total_texts\")\n",
    "        \n",
    "        # removing non ascii chars\n",
    "        reference_correction = reference_corrections[text_id]\n",
    "        \n",
    "        if remove_nonprint:\n",
    "            reference_correction = ''.join(filter(lambda x: x in string.printable, str(reference_correction)))\n",
    "            \n",
    "        if reference_correction == \"NA\":\n",
    "            increment_counter(counters, \"reference_na\")\n",
    "            \n",
    "        if text_id in candidate_corrections:\n",
    "            increment_counter(counters, \"total_system_texts\")\n",
    "            candidate = candidate_corrections[text_id]\n",
    "            \n",
    "            if remove_nonprint:\n",
    "                candidate = ''.join(filter(lambda x: x in string.printable, candidate))\n",
    "                \n",
    "            if candidate == \"NA\":\n",
    "                increment_counter(counters, \"system_provided_na\")\n",
    "                \n",
    "            # matching NA counts as 1\n",
    "            if reference_correction == \"NA\" and candidate == \"NA\":\n",
    "                increment_counter(counters, \"system_provided_correct_na\")\n",
    "                continue\n",
    "                \n",
    "            # Run provided \"NA\" when a correction was required (=> 0)\n",
    "            # or Run provided a correction when \"NA\" was required (=> 0)\n",
    "            if candidate == \"NA\" or reference_correction == \"NA\":\n",
    "                continue\n",
    "                \n",
    "            # remaining case is both reference and candidate are not \"NA\"\n",
    "            # both are inserted/added for ROUGE/BLEURT/etc. computation\n",
    "            references.append(reference_correction)\n",
    "            predictions.append(candidate)\n",
    "    \n",
    "\n",
    "    \n",
    "    return references, predictions, counters\n",
    "\n",
    "submission_file = \"EA_shared_task_outputs/case2_prompt_with_RAG/final_csv_validation_ms.txt\"\n",
    "reference_csv_file = \"MEDIQA-CORR-2024-MS-ValidationSet-1-Full.csv\"\n",
    "\n",
    "\n",
    "reference_corrections, reference_flags, reference_sent_id = parse_reference_file(reference_csv_file)\n",
    "candidate_corrections, candidate_flags, candidate_sent_id = parse_run_submission_file(submission_file)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_results = compute_accuracy(reference_flags, reference_sent_id, candidate_flags, candidate_sent_id)\n",
    "print(\"Accuracy Results:\\n\", accuracy_results)\n",
    "print()\n",
    "\n",
    "# NLG Eval for corrections\n",
    "references, predictions, counters = get_nlg_eval_data(reference_corrections, candidate_corrections)\n",
    "metrics = NLGMetrics()\n",
    "nlg_eval_results = metrics.compute(references, predictions, counters) \n",
    "\n",
    "\n",
    "print(\"NLG Eval Results:\\n\", nlg_eval_results) \n",
    "print()\n",
    "\n",
    "# debug check\n",
    "print(counters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7870cdd4-1bec-4fc9-96a6-5dbc7b9e9ca5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Evaluation Script 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aece150-af88-4a31-a5db-bdc8b05cb319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c00f481-8d14-487e-9230-1913da3157cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bleurt'...\n",
      "remote: Enumerating objects: 134, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 134 (delta 0), reused 17 (delta 0), pack-reused 116\u001b[K\n",
      "Receiving objects: 100% (134/134), 31.28 MiB | 35.67 MiB/s, done.\n",
      "Resolving deltas: 100% (49/49), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/google-research/bleurt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e2005-b9ea-47b1-9bb6-8f93ea22daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd bleurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f106909-dec1-4195-aa7f-f686a8bfd383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd060ac-7292-4c22-9917-fd342dc02351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32deb3af-63c6-45d2-a793-b6105530a80c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip BLEURT-20.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe82b30-7da6-4899-a6a2-298b327973a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import bert_score.score as bertscore\n",
    "import bleurt.score as bleurtscore\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc87461c-8e3a-4479-a453-996ec40233fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Parsing Funcs #\n",
    "#################\n",
    "\n",
    "def parse_reference_file(filepath):\n",
    "    \"\"\"Parsing reference file path.\n",
    "\n",
    "    Returns:\n",
    "        reference_corrections (dict) {text_id: \"reference correction\"}\n",
    "        reference_flags (dict) {text_id: \"1 or 0 error flag\"}\n",
    "        reference_sent_id (dict) {text_id: \"error sentence id or -1\"}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    reference_corrections = {}\n",
    "    reference_flags = {}\n",
    "    reference_sent_id = {}\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text_id = row['Text ID']\n",
    "        corrected_sentence = row['Corrected Sentence']\n",
    "        \n",
    "        if not isinstance(corrected_sentence, str):\n",
    "            if math.isnan(corrected_sentence):\n",
    "                corrected_sentence = \"NA\"\n",
    "            else:\n",
    "                corrected_sentence = str(corrected_sentence)\n",
    "                corrected_sentence = corrected_sentence.replace(\"\\n\", \" \") \\\n",
    "                  .replace(\"\\r\", \" \").strip()\n",
    "                  \n",
    "        reference_corrections[text_id] = corrected_sentence\n",
    "        reference_flags[text_id] = str(row['Error Flag'])\n",
    "        reference_sent_id[text_id] = str(row['Error Sentence ID'])\n",
    "\n",
    "    return reference_corrections, reference_flags, reference_sent_id\n",
    "\n",
    "\n",
    "def parse_run_submission_file(filepath):\n",
    "    \n",
    "    file = open(filepath, 'r')\n",
    "\n",
    "    candidate_corrections = {}\n",
    "    predicted_flags = {}\n",
    "    candidate_sent_id = {}\n",
    "    \n",
    "    lines = file.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "            \n",
    "        if not re.fullmatch('[a-z0-9\\-]+\\s[0-9]+\\s\\-?[0-9]+\\s.+', line):\n",
    "            print(\"Invalid line: \", line)\n",
    "            continue\n",
    "            \n",
    "        # replacing consecutive spaces\n",
    "        line = re.sub('\\s+', line, ' ')\n",
    "        \n",
    "        # parsing\n",
    "        items = line.split()\n",
    "        text_id = items[0]\n",
    "        error_flag = items[1]\n",
    "        sentence_id = items[2]\n",
    "        corrected_sentence = ' '.join(items[3:]).strip()\n",
    "        \n",
    "        # debug - parsing check\n",
    "        # print(\"{} -- {} -- {} -- {}\".format(text_id, error_flag, sentence_id, corrected_sentence))\n",
    "\n",
    "        predicted_flags[text_id] = error_flag\n",
    "        candidate_sent_id[text_id] = sentence_id\n",
    "\n",
    "        # processing candidate corrections\n",
    "        # removing quotes\n",
    "\n",
    "        while corrected_sentence.startswith('\"') and len(corrected_sentence) > 1:\n",
    "            corrected_sentence = corrected_sentence[1:]\n",
    "            \n",
    "        while corrected_sentence.endswith('\"') and len(corrected_sentence) > 1:\n",
    "            corrected_sentence = corrected_sentence[:-1]\n",
    "                   \n",
    "        if error_flag == '0':\n",
    "            # enforcing \"NA\" in predicted non-errors (used for consistent/reliable eval)\n",
    "            candidate_corrections[text_id] = \"NA\"\n",
    "        else:\n",
    "            candidate_corrections[text_id] = corrected_sentence\n",
    "\n",
    "    return candidate_corrections, predicted_flags, candidate_sent_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d9fa63-0500-4add-b94c-5d0ae53d7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Eval Funcs #\n",
    "##############\n",
    "\n",
    "def compute_accuracy(reference_flags, reference_sent_id, predicted_flags, candidate_sent_id):\n",
    "    # Error Flags Accuracy (missing predictions are counted as false)\n",
    "    matching_flags_nb = 0\n",
    "    \n",
    "    for text_id in reference_flags:\n",
    "        if text_id in predicted_flags and reference_flags[text_id] == predicted_flags[text_id]:\n",
    "            matching_flags_nb += 1\n",
    "            \n",
    "    flags_accuracy = matching_flags_nb / len(reference_flags)\n",
    "    \n",
    "    # Error Sentence Detection Accuracy (missing predictions are counted as false)\n",
    "    matching_sentence_nb = 0\n",
    "    \n",
    "    for text_id in reference_sent_id:\n",
    "        if text_id in candidate_sent_id and candidate_sent_id[text_id] == reference_sent_id[text_id]:\n",
    "            matching_sentence_nb += 1\n",
    "            \n",
    "    sent_accuracy = matching_sentence_nb / len(reference_sent_id)\n",
    "\n",
    "    return {\n",
    "        \"Error Flags Accuracy\": flags_accuracy,\n",
    "        \"Error Sentence Detection Accuracy\": sent_accuracy\n",
    "    }\n",
    "\n",
    "def increment_counter(counters, counter_name):\n",
    "    counters[counter_name] = counters[counter_name] + 1\n",
    "\n",
    "def clip(value): # clip to a 0-1 value\n",
    "    return max(0, min(1, value))\n",
    "\n",
    "class NLGMetrics(object):\n",
    "\n",
    "    def __init__(self, metrics = ['ROUGE', 'BERTSCORE', 'BLEURT']): ## default metrics\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def compute(self, references, predictions, counters):\n",
    "        results = {}\n",
    "\n",
    "        assert len(predictions) == len(references), \"Predictions and references do not have the same size.\"\n",
    "        \n",
    "        results['aggregate_subset_check'] = np.array([0 for x in range(len(predictions))])\n",
    "        aggregate_components = 0\n",
    "\n",
    "        if 'ROUGE' in self.metrics:\n",
    "            rouge = Rouge() \n",
    "            rouge_scores = rouge.get_scores(predictions, references)\n",
    "                            \n",
    "            rouge1f_scores = []\n",
    "            rouge2f_scores = []\n",
    "            rougeLf_scores = []\n",
    "            \n",
    "            for i in range(len(references)):\n",
    "                r1f = rouge_scores[i][\"rouge-1\"][\"f\"]\n",
    "                r2f = rouge_scores[i][\"rouge-2\"][\"f\"]\n",
    "                rlf = rouge_scores[i][\"rouge-l\"][\"f\"]\n",
    "                \n",
    "                rouge1f_scores.append(r1f)\n",
    "                rouge2f_scores.append(r2f)\n",
    "                rougeLf_scores.append(rlf)\n",
    "                \n",
    "            # for checking comparison with composite\n",
    "            rouge1check = np.array(rouge1f_scores).mean()\n",
    "            rouge2check = np.array(rouge2f_scores).mean()\n",
    "            rougeLcheck = np.array(rougeLf_scores).mean()\n",
    "\n",
    "            results['R1F_subset_check'] = rouge1check\n",
    "            results['R2F_subset_check'] = rouge2check\n",
    "            results['RLF_subset_check'] = rougeLcheck\n",
    "\n",
    "            # rouge-1-f is used for the aggregate score\n",
    "            # sum element-wise for later aggregate score computation\n",
    "            results['aggregate_subset_check'] = results['aggregate_subset_check'] + np.array(rouge1f_scores)\n",
    "            aggregate_components += 1\n",
    "            \n",
    "            ###############################\n",
    "            # Composite score computation #\n",
    "            ###############################\n",
    "            \n",
    "            \"\"\"\n",
    "            NLG METRIC on sentence vs. sentence cases + ones or zeros \n",
    "            when either the reference or the candidate correction is NA\n",
    "            \"\"\"\n",
    "            \n",
    "            rouge1score = np.array(rouge1f_scores).sum()\n",
    "            rouge2score = np.array(rouge2f_scores).sum()\n",
    "            rougeLscore = np.array(rougeLf_scores).sum()\n",
    "            \n",
    "            composite_score_rouge1 = (rouge1score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "            composite_score_rouge2 = (rouge2score + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "            composite_score_rougeL = (rougeLscore + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "\n",
    "            results['R1FC'] = composite_score_rouge1\n",
    "            results['R2FC'] = composite_score_rouge2\n",
    "            results['RLFC'] = composite_score_rougeL\n",
    "\n",
    "        if 'BERTSCORE' in self.metrics:\n",
    "            bertScore_Precision, bertScore_Recall, bertScore_F1 = bertscore(predictions, references, model_type='microsoft/deberta-xlarge-mnli', lang='en', device ='cpu' , verbose=True, rescale_with_baseline=True) # roberta-large\n",
    "            \n",
    "            bertscores = bertScore_F1.numpy()\n",
    "            ## clip scores to [0,1]\n",
    "            bertscores = np.array([clip(num) for num in bertscores])\n",
    "\n",
    "            results['BERTSCORE_subset_check'] = bertscores.mean()\n",
    "            composite_score_bert = (bertscores.sum() + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "            results['BERTC'] = composite_score_bert\n",
    "\n",
    "            # sum element-wise for later aggregate score computation\n",
    "            results['aggregate_subset_check'] = results['aggregate_subset_check'] + np.array(bertscores)\n",
    "            aggregate_components += 1\n",
    "\n",
    "            \n",
    "        if 'BLEURT' in self.metrics:\n",
    "            bleurtscorer = bleurtscore.BleurtScorer(checkpoint=\"BLEURT-20\")\n",
    "            \n",
    "            bleurtscores = bleurtscorer.score(references=references, candidates=predictions, batch_size =1)\n",
    "            ## clip scores to [0,1]\n",
    "            bleurtscores = np.array([clip(num) for num in bleurtscores])\n",
    "\n",
    "            results['BLEURT_subset_check'] = bleurtscores.mean()\n",
    "            composite_score_bleurt = (bleurtscores.sum() + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "            results['BLEURTC'] = composite_score_bleurt\n",
    "\n",
    "            # sum element-wise for later aggregate score computation\n",
    "            results['aggregate_subset_check'] = results['aggregate_subset_check'] + np.array(bleurtscores) \n",
    "                    \n",
    "            aggregate_components += 1\n",
    "\n",
    "        if aggregate_components > 0:\n",
    "            aggregate_subset_scores = results['aggregate_subset_check'] / aggregate_components\n",
    "            composite_score_agg = (aggregate_subset_scores.sum() + counters[\"system_provided_correct_na\"]) / counters[\"total_texts\"]\n",
    "            \n",
    "            results['aggregate_subset_check'] = aggregate_subset_scores.mean() \n",
    "            results['AggregateC'] = composite_score_agg\n",
    "\n",
    "            \n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "def get_nlg_eval_data(reference_corrections, candidate_corrections, remove_nonprint = False):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    \n",
    "    counters = {\n",
    "        \"total_texts\": 0,\n",
    "        \"reference_na\": 0,\n",
    "        \"total_system_texts\": 0,\n",
    "        \"system_provided_na\": 0,\n",
    "        \"system_provided_correct_na\": 0,\n",
    "    }\n",
    "    \n",
    "    for text_id in reference_corrections:\n",
    "        increment_counter(counters, \"total_texts\")\n",
    "        \n",
    "        # removing non ascii chars\n",
    "        reference_correction = reference_corrections[text_id]\n",
    "        \n",
    "        if remove_nonprint:\n",
    "            reference_correction = ''.join(filter(lambda x: x in string.printable, str(reference_correction)))\n",
    "            \n",
    "        if reference_correction == \"NA\":\n",
    "            increment_counter(counters, \"reference_na\")\n",
    "            \n",
    "        if text_id in candidate_corrections:\n",
    "            increment_counter(counters, \"total_system_texts\")\n",
    "            candidate = candidate_corrections[text_id]\n",
    "            \n",
    "            if remove_nonprint:\n",
    "                candidate = ''.join(filter(lambda x: x in string.printable, candidate))\n",
    "                \n",
    "            if candidate == \"NA\":\n",
    "                increment_counter(counters, \"system_provided_na\")\n",
    "                \n",
    "            # matching NA counts as 1\n",
    "            if reference_correction == \"NA\" and candidate == \"NA\":\n",
    "                increment_counter(counters, \"system_provided_correct_na\")\n",
    "                continue\n",
    "                \n",
    "            # Run provided \"NA\" when a correction was required (=> 0)\n",
    "            # or Run provided a correction when \"NA\" was required (=> 0)\n",
    "            if candidate == \"NA\" or reference_correction == \"NA\":\n",
    "                continue\n",
    "                \n",
    "            # remaining case is both reference and candidate are not \"NA\"\n",
    "            # both are inserted/added for ROUGE/BLEURT/etc. computation\n",
    "            references.append(reference_correction)\n",
    "            predictions.append(candidate)\n",
    "    \n",
    "    return references, predictions, counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc427fba-0c5f-4a52-b1d7-feff49899514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission_file = \"final_csv_validation_ms.txt\"\n",
    "reference_csv_file = \"MEDIQA-CORR-2024-MS-ValidationSet-1-Full.csv\"\n",
    "\n",
    "reference_corrections, reference_flags, reference_sent_id = parse_reference_file(reference_csv_file)\n",
    "candidate_corrections, candidate_flags, candidate_sent_id = parse_run_submission_file(submission_file)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_results = compute_accuracy(reference_flags, reference_sent_id, candidate_flags, candidate_sent_id)\n",
    "print(\"Accuracy Results:\\n\", accuracy_results)\n",
    "print()\n",
    "\n",
    "# NLG Eval for corrections\n",
    "references, predictions, counters = get_nlg_eval_data(reference_corrections, candidate_corrections)\n",
    "metrics = NLGMetrics(metrics = ['ROUGE', 'BERTSCORE', 'BLEURT']) # metrics = ['ROUGE', 'BERTSCORE', 'BLEURT']\n",
    "nlg_eval_results = metrics.compute(references, predictions, counters) \n",
    "\n",
    "\n",
    "print(\"NLG Eval Results:\\n\", nlg_eval_results) \n",
    "print()\n",
    "\n",
    "# debug check\n",
    "print(counters)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
